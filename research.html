<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research</title>
    <link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body>
    <header>
      <h1>My Research</h1>
    </header>

    <nav>
    <ul>
      <li><a href="/JumpDragon/">Home</a></li>
      <li><a href="/JumpDragon/research.html">Research</a></li>
      <li><a href="/JumpDragon/projects.html">Projects</a></li>
      <li><a href="/JumpDragon/hobbies.html">Hobbies</a></li>
      <li><a href="/JumpDragon/contact.html">Contact</a></li>
    </ul>
  </nav>

  <section>
    <h1>Current Research</h1>
    <h4>Cognitive Systems Lab</h4>
    <img src="https://www.insightintodiversity.com/wp-content/uploads/2021/04/uw-madison-1.png" alt="UW-Madison" width="400" height="225">
    <p>I'm continuing my research on Automated Vehicle (AV) technology. Currently, we are working on our research <br>
    studying Vehicle-Driver interaction, specifically on notification and communication systems between driver and AV.<br>
    I'm utilizing the STM package in R to unbiasly determine main concerns drivers have in different weather conditions <br>
    and driving automation levels. We will have a paper to submit for HFES 2024 and look forward to presenting.
     </p>

    <h1>Past Research</h1>
    <h4>MBZUAI</h4>
    <img src="https://static.youthop.com/uploads/2022/12/opp_4017.jpg" alt="MBZUAI" width="400" height="200">
    <p>As a Machine Learning Research Intern at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) <br>
    I researched under Prof. Chih-Jen Lin, a Distinguished Professor of Computer Science at National Taiwan University, <br>
    and Affiliated Professor of Machine Learning at MBZUAI. </p>

    <p><b>This research focused on Extreme Multi-Label  & Long Document Text Classification.</b> Current deep learning methods utilize pretrained <br>
    models such as <b>BERT</b> to process texts, however <b>BERT</b> has a 512 token limit, which means vital context may be lost. <br>
    We want to process large documents, but <b>BERT</b> may not be the best option. In fact, <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/text_classification_baseline/text_classification_baseline.pdf">linear classifiers</a> <br>
    are extremely competitive and often outperform deep learning models in accuracy and speed. The speed factor is especially notable, <br>
    as it can turn hours of computing time into mere seconds. Another advantage of linear classifiers is that these SVM's can be <br>
    run on CPU. As GPU's are in high demand, they shouldn't be overlooked. </p>

    <p>Although competitive, linear classifiers do fail sometimes. The advantage of using deep learning models is the ability to <br>
    incorporate context in our decision making process. Linear classifiers utilize bag-of-words technique - basically the words <br>
    aren't able to talk to each other - whereas <b>BERT</b> utilizes <a href="https://arxiv.org/pdf/1706.03762.pdf">attention</a>.</p>

    <p>We want to know how to work around <b>BERT</b>'s 512 token limitation to analyze large documents. <a href="https://arxiv.org/pdf/2203.11258.pdf">Papers</a> have proposed that <br>
    512 tokens is plenty of context, and out-of-the-box <b>BERT</b> often outperforms finetuned models or models like LongFormer <br>
    which can take 4000+ tokens depending on the model. Another option is <b>BELT</b> - <b>BE</b>rt <b>F</b>or <b>L</b>onger <b>T</b>exts. Built on <b>BERT</b> framework, <br>
    <b>BELT</b> is something we looked into. <a href="https://github.com/mim-solutions/bert_for_longer_texts"><b>BELT</b></a>, being built on <b>BERT</b> framework, gives us <br>
    access to all <b>BERT</b> documentation. </p>

    <p>Our main contribution which connot be overlooked is the creation and preprocessing of novel, long document datasets. <br>
    These datasets are incredibly hard to come by and clean, so there was significant manpower dedicated to these datasets. <br>
    We also found that <b>BERT</b> underperformed in every dataset compaired to linear SVMs. There were only two datasets in <br>
    which a hierarchical, fine tuned <b>BERT</b> model outperformed the linear SVM and it wasn't by much. Because some datasets <br>
    were smaller than others, <b>BERT</b>'s variability was extremely high between tests, making it an extremely poor baseline. <br>
    We want baselines to be consistent, quick, and replicable which <b>BERT</b> cannot offer. Not only does it take days to tune, <br>
    hierarchical <b>BERT</b> also shows extreme variability in F1-scores. </p>

    <p>To conclude, I learned a lot from this internship and had a blast collaborating with such a diverse group. I was glad to <br>
    represent the America's among the North Africans, Europeans, East, West, and South Asians that were all in attendance <br>
    and I hope MBZUAI finds success.</p>

    <h4>Cognitive Systems Lab</h4>
    <p><i>Incorporating driver expectations into a taxonomy of transfers of control for automated vehicles</i></p>

    <p><a href="https://journals.sagepub.com/doi/pdf/10.1177/1071181322661241">This research paper</a> defines all the main transfers of control of the vehicle between driver and AV. This is also my first <br>
    research paper! We utilized Zotero, Obsidian, and the Zettelkasten method to create a web of interconnected resources, to quickly <br>
    produce quality research, which we still utilize today.</p>
  </section>

  <footer>
    <p>&copy; 2023 John Douglas. All rights reserved.</p>
  </footer>
</body>
</html>